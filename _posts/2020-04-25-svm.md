---
layout: post
title: "SVM "
date: 2020-04-25T15:19:43.401Z
updated_date: 2020-04-25T15:19:43.417Z
published: false
tags:
  - machinelearning
  - datascience
  - python
  - sklearn
categories:
  - machinelearning
  - datascience
  - python
  - sklearn
show_ads: false
include_mathjax: true
---
SVM or support vector machines are supervised learning models that analyze data and recognize patterns on its own. They are used for both classification and regression analysis.

SVM model is the representation of the dataset as points in space, so that the example of the separate categories are divided by a clear gap which is as wide as possible.

Any new incoming data is then mapped to one of these few categories based on which side of the gap they fall on.

{% include lazyload.html image_src="https://i.ibb.co/XYgV0Yg/Screenshot-2020-04-25-at-9-02-13-PM.png" image_alt="Two dimensional SVM example" image_title="Two dimensional SVM example" %}

For example, in the above image we can clearly see that there are two categories in the dataset. Category blue and category pink.

Our aim to differentiate between the two categories. One simple way of doing that is to draw a line in between the two categories. But as we can see, there are a lot of lines which can clearly divide dataset into two parts.

What we actually do is that we choose a hyperplane that maximizes the margin between the classes. The data points (Vectors) touching the two outer lines are called support vectors.

This simple example of two dimension linear plots can be further used in a dataset having further more dimensions. Each time our idea will be to draw a hyperplane which can divide the data into different categories.

Now we are going to talk about some related mathematics and discuss different terms related to SVM.

In general the discussion of SVM is divided into three parts according to how SVM evolved.

* Maximal Margin Classifier
* Support Vector Classifier
* Support Vector Machine

We will slowly move the article toward the Support Vector Machine but for proper understanding of SVM's we have to go through, Maximal Margin Classifier and Support Vector Classifier.

## Maximal Margin Classifier

Maximal Margin Classifier is a model which is used to classify the observations into two parts using a hyperplane.

### What is a Hyperplane?

Simply put a hyperplane is a subspace in p-dimensional space having `p -1` dimensions. For example, in two-dimensional space the hyperplane will be of 1 dimension, or it will be a line. Similarly, in case of 3 dimensions it will be a two-dimensional plane.

In two dimensions the equation of the hyperplane are given by,

{% include math.html math_code="$\beta_0 + \beta_1X_1 + \beta_2X_2 = 0$" style="margin-top:0.2em;" %}

{% include math.html math_code="$where\ vector\ (X1,\ X2)\ is\ on\ the\ hyperplane$" %}

We can also find some similarity of this equation with the equation of line.

Its fairly easy to extend this equation and find the equation of a hyperplane in `p` dimensions.

{% include math.html math_code="$\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0$" style="margin-top:0.2em;" %}

Now if,

{% include math.html math_code="$\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0 > 0$" style="margin-top:0.2em;" %}

Then the vector is on the one side of the hyperplane and if,

{% include math.html math_code="$\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p = 0 < 0$" style="margin-top:0.2em;" %}

The vector is on the other side of the plane.

To sum it up, our main aim in case of Maximal Margin Classifier is to create a hyperplane fitted on a training data of `n X p` matrix `X`, containing `n` training observations in `p`-dimensional space such that all these vector falls in one of the two classes divided by the hyperplane.

If we represent the classes(labels) for all the n values,

{% include math.html math_code="$y_1, ..., y_n\ \epsilon \{-1, 1\}$" style="margin-top:0.2em;" %}

Where -1 represent one class and 1 represent the other class.

Our main aim for any incoming test vector,

{% include math.html math_code="$x^* = (x_1^*\ ...\ x_p^*)^T$" style="margin-top:0.2em;" %}

is that our model have to allot this incoming test vector to one of the two classes. This equation given the class of the incoming test vector.

{% include math.html math_code="$f_x^* = \beta_0 + \beta_1x_{1}^* + \beta_2x_{2}^* + ... + \beta_px_{p}^*$" style="margin-top:0.2em;" %}

If the value of this function is positive, we assign to class 1, otherwise we assign it to class -1.

A simple issue in this approach is that there are infinite number of hyperplanes possible that can divide a perfect distribution.

{% include lazyload.html image_src="https://i.ibb.co/Q9X5Mdr/Screenshot-2020-04-26-at-11-50-26-PM.png" image_alt="Infinite possible hyerplanes in SVM" image_title="Infinite possible hyerplanes in SVM" %}

The problem reduces to choosing the best hyperplane possible which divides the observations into two parts.

A natural choice is find the perpendicular distance of each observation from the  potential hyperplanes, the one which produces the maximum `margin` from both the sides is chosen as the hyperplane.

{% include lazyload.html image_src="https://i.ibb.co/XYgV0Yg/Screenshot-2020-04-25-at-9-02-13-PM.png" image_alt="SVM: Choosing a hyperplane" image_title="SVM: Choosing a hyperplane" %}

Once we have the hyperplane, it is fairly easy to predict the classes of test observations.

The only assumption that we are making here is that a hyperplane dividing the observations in the training set will also divide the observations in the test set, which is not always true. Therefore, this model can lead to overfitting when `p` is large.

As we have already discussed that the points on dashed line are called support vectors, it has been found that the position of hyperplane only depends on support vector and is not dependent on the other observations in the dataset.

This is how we define a Maximal margin classifier. There are a few issues with 